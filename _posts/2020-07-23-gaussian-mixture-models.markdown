---
layout: post
title:  "An overview of Gaussian Mixture Models"
date:   2020-07-23 09:00:00 +0000
description: Quick overview of Gaussian Mixture Models (GMMs) with an intuitive introduction and python examples.
author: Massimiliano Patacchiola
type: bayesian methods
comments: false
published: false
---


In this post I will give you an overview of Gaussian Mixture Models (GMMs) starting from an intuitive introduction with some examples, passing through the mathematical details, and concluding with a Python implementation. This post is based on Chapter 11 of the book  *"Mathematics for Machine Learning"* by Deisenroth, Faisal, and Ong which is available in PDF [here](https://mml-book.com) and in the paperback version [here](https://www.cambridge.org/gb/academic/subjects/computer-science/pattern-recognition-and-machine-learning/mathematics-machine-learning?format=PB).

![Mathematics for Machine Learning book]({{site.baseurl}}/images/books_math_for_machine_learning.png){:class="img-responsive"}

**What do you need to know?** In order to enjoy the post you need to know some basic probability theory (random variables, probability distributions, etc), some calculus (derivatives and gradients), and some Python if you want to delve into the programming part. The post follows this plot:

- A quick recap on Gaussian distributions
- Fitting a Gaussian via Maximum Likelihood Estimation (MLE)
- Problem: a single Gaussian cannot fit multimodal data
- Solution: use Gaussian Mixture Models (GMMs)
- Problem: MLE cannot be used on GMMs
- Solution: use Expectation Maximization (EM)


Quick recap on Gaussian distributions
-------------------------------------

Since we are going to extensively use Gaussian distributions I will present here a quick recap. A Gaussian distribution is a continuous [probability distribution](https://en.wikipedia.org/wiki/Probability_distribution) that is characterized by its symmetrical bell shape. A **univariate Gaussian** distribution is defined as follows:

$$
p\left(x \mid \mu, \sigma^{2}\right)= \mathcal{N}\left(\mu, \sigma^{2}\right) = \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right).
$$

Note that $$\mu$$ and $$\sigma$$ are scalars representing the mean and standard deviation of the distribution. 
We like Gaussians because they have several nice properties, for instance marginals and conditionals of Gaussians are still Gaussians. Thanks to these properties Gaussian distributions have been widely used in a variety of algorithms and methods, such as the [Kalman filter](https://en.wikipedia.org/wiki/Kalman_filter) and [Gaussian processes](https://en.wikipedia.org/wiki/Gaussian_process).

The univariate Gaussian defines a distribution over a single random variable, but in many problems we have multiple random variables thus we need a version of the Gaussian which is able to deal with this multivariate case. The **multivariate Gaussian** distribution can be defined as follows:

$$
p(\boldsymbol{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \mathcal{N}\left(\boldsymbol{\mu}, \boldsymbol{\Sigma}\right) = (2 \pi)^{-\frac{D}{2}}|\boldsymbol{\Sigma}|^{-\frac{1}{2}} \exp \left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right).
$$

Note that $$\boldsymbol{\mu}$$ and $$\boldsymbol{\Sigma}$$ are not scalars but a vector (of means) and a matrix (of variances). 
The value $$|\boldsymbol{\Sigma}|$$ is the [determinant](https://en.wikipedia.org/wiki/Determinant) of $$\boldsymbol{\Sigma}$$, and $$D$$ is the number of dimensions $$\boldsymbol{x} \in \mathbb{R}^{D}$$. Interpolating over $$\boldsymbol{\mu}$$ has the effect of shifting the Gaussian on the $$D$$-dimensional (hyper)plane, whereas changing the matrix $$\boldsymbol{\Sigma}$$ has the effect of changing the shape of the Gaussian.

Likelihood of a Gaussian
-------------------------

Let's suppose we are given a bunch of data and we are interested in finding a distribution that fits this data. We can assume that the data has been generated by an underlying process, and that we want to model this process. We can choose a Gaussian distribution as model for our data. The goal now is to find mean and variance of the Gaussian. This can be done via **Maximum Likelihood (ML)** estimation. 

For simplicity, suppose we want to estimate the mean $$\mu$$ of a univariate Gaussian distribution (we suppose the variance is known), given a dataset of points $$\mathcal{X}= \{x_{n} \}_{n=1}^{N}$$ we proceed as follows: (i) define the [likelihood](https://en.wikipedia.org/wiki/Likelihood_function) (predictive distribution of the training data given the parameters) $$p(\mathcal{X} \vert \mu)$$, (ii) evaluate the log-likelihood $$\mathcal{L}_{\mu}$$, and (iii) find the derivative of the log-likelihood with respect to $$\mu$$. We are under the assumption of [independent and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables) (i.i.d.) random variables. In analytical form the three steps are:

$$
\begin{aligned}
p(\mathcal{X} \vert \mu) &=\prod_{n=1}^{N} \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp -\frac{\left(x_{n}-\mu\right)^{2}}{2 \sigma^{2}}, \\
\mathcal{L} &=\sum_{n=1}^{N}\left[\log \left(\frac{1}{\sqrt{2 \pi \sigma^{2}}}\right)-\frac{\left(x_{n}-\mu\right)^{2}}{2 \sigma^{2}}\right], \\
\frac{d \mathcal{L}}{d \mu}  &=\sum_{n=1}^{N} \frac{x_{n}-\mu}{\sigma^{2}}.
\end{aligned}
$$

Note that applying the log to the likelihood in the second step, turned products into sums and helped us getting rid of the exponential. The third step has given us the derivative of the log-likelihood, what we need to do now is to set the derivative to zero and solve with respect to the parameter of interest $$\mu$$, this lead to the ML estimator for the mean of a univariate Gaussian:

$$
\mu_{\text{ML}}=\frac{\sum_{n=1}^{N} x_{n}}{N}.
$$

The equation above just says that the ML estimate of the mean can be obtained by summing all the values then divide by the total number of points. The ML estimate of the variance can be calculated with a similar procedure, starting from the log-likelihood and differentiating with respect to $$\sigma$$:

$$
\sigma_{\text{ML}}^{2}=\frac{\sum_{n=1}^{N} (x_{n} - \mu)^{2}}{N}.
$$

Example: fitting distributions
----------------------------------------

**Fitting unimodal distributions.** Let's consider a simple example and let's write some Python code for it. Suppose we have a set of data that has been generated by an underlying (unknown) distribution. For this example, I will consider the body measurements dataset provided by Heinz et al. (2003) that you can [download from my repository](https://gist.github.com/mpatacchiola/9f91bddb09ddf9a53627d054f9bc9a48). In particular, I will gather the subset of body weight (in kilograms). Once we have the data, we would like to estimate the mean and standard deviation of a Gaussian distribution by using ML. This can be easily done by plugging the closed-form ML expressions of mean and standard deviation into our code:

```python
import numpy as np

data = np.genfromtxt('./bdims.csv', delimiter=',', skip_header=1)[,-3]
N = len(data)
mean = data.sum() / N 
std = np.sqrt(np.sum((data - mean)**2) / N)
print("{mean:.3f} +- {std:.3f} (N={N:d})".format(mean=mean, std=std, N=N))
```

The snippet will print:

```
69.148 +- 13.333 (N=507)
```

which seems to be a good approximation of the true underlying distribution give the 507 measurements. For the [law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers), as the number of measurements increases the estimation of the true underlying parameters gets more precise. We can assign the data points to an histogram of 15 bins and visualize the raw distribution. Then we can plot the Gaussian estimated via ML to verify how does it fit:

![body weight]({{site.baseurl}}/images/gmm_weight.png){:class="img-responsive"}


**Fitting multimodal distributions.** In the previous example we supposed that the data was generated by a single Gaussian distribution. However, it is likely that there are other factors of variation that we did not consider. For instance, we know that height is correlated with weight and sex, therefore it is possible that there are sub-populations we did not capture using a single Gaussian model.
How can we solve this problem?


Gaussian Mixture Models (GMMs)
--------------------------------------

Here is an idea, what if instead of using a single Gaussian distribution we use multiple Gaussians instead? Each Gaussian would have its own mean and variance and we could mix them by adjusting some proportional coefficients. This would be like mixing different sounds by using the sliders on a console. Something like this is known as a Gaussian Mixture Model (GMM). A GMM with $$K$$ **univariate** Gaussian components is defined as

$$
\mathcal{N}\left(\mu_{k}, \sigma_{k} \right) = \sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(x \mid \mu_{k}, \sigma_{k}\right)
\quad \text{where} 
$$

$$
0 \leqslant \pi_{k} \leqslant 1, \quad \sum_{k=1}^{K} \pi_{k}=1
\quad \text{and} \quad
\boldsymbol{\theta}=\left\{\mu_{k}, \sigma_{k}, \pi_{k} \right\}_{k=1}^{K}
$$

Similarly we can define a GMM for the **multivariate** case:

$$
\mathcal{N}\left(\boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k} \right) = \sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\boldsymbol{x} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right) 
$$

under identical constraints for $$\pi$$ and with $$\boldsymbol{\theta}=\left\{\boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}, \pi_{k} \right\}_{k=1}^{K}$$. GMMs are more expressive than simple Gaussians and they are often able to capture subtle differences in the data. However, the problem of optimal data fit is still not solved since we generally do not know the form of the underlying distribution, therefore also a GMM can be a poor fit (e.g. if we choose the wrong number of components). 


**Important**: GMMs are the weighted sum of Gaussian densities. This is different from the weighted sum of Gaussian random variables. For instance, given two Gaussian random variables $$\boldsymbol{x}$$ and $$\boldsymbol{y}$$, their weighted sum is defined as

$$
p(a \boldsymbol{x}+b \boldsymbol{y})=\mathcal{N}\left(a \boldsymbol{\mu}_{x}+b \boldsymbol{\mu}_{y}, a^{2} \boldsymbol{\Sigma}_{x}+b^{2} \boldsymbol{\Sigma}_{y}\right).
$$

Likelihood of a GMM
--------------------

In the following, I detail how to obtain a maximum likelihood estimate of a single univariate Gaussian mean $$\mu_{k}$$ which is part of a GMM with $$K$$ components. In doing so we have to follow the same procedure adopted for estimating the mean of the univariate Gaussian, which is done in three steps: (i) define the likelihood, (ii) estimate the log-likelihood, (iii) find the partial derivative of the log-likelihood with respect to $$\mu_{k}$$. In short:

$$
\begin{aligned}
p(\mathcal{X} \mid \theta) &= \prod_{n=1}^{N} \sum_{k=1}^{K} \pi_{k} \frac{1}{\sqrt{2 \pi \sigma_{k}^{2}}} \exp -\frac{\left(x_{n}-\mu_{k} \right)^{2}}{2 \sigma_{k}^{2}}, \\
\mathcal{L} &= \sum_{n=1}^{N} \log \Bigg[ \sum_{k=1}^{K} \pi_{k} \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp -\frac{\left(x_{n}-\mu_{k}\right)^{2}}{2 \sigma_{k}^{2}} \Bigg], \\
\frac{\partial \mathcal{L}}{\partial \mu_{k}} &= \sum_{n=1}^{N} \frac{\pi_{k} \mathcal{N}\left(x_{n} \mid \mu_{k}, \sigma_{k}\right)}{\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left(x_{n} \mid \mu_{j}, \sigma_{j}\right)} \cdot \frac{x_{n}-\mu_{k}}{\sigma_{k}^{2}}. \\
\end{aligned}
$$

If you compare the equations above with the equations of the univariate Gaussian you will notice that in the second step there is an an additional factor: the summation over the $$K$$ components. This summation is problematic since it prevents the log function from being applied to the normal densities. Unlike the log of a product, the log of a sum does not immediately simplify. As a result the partial derivative in the third step does not lead to a closed-form expression for $$\mu_{k}$$ which now depends on the $$K$$ means, variances, and mixture weights.


Responsibilities
----------------

How can we find the parameters of a GMM if we do not have a closed-form expression for the ML estimator? To answer this question, we need to introduce the concept of responsibility. It is possible to immediately catch what responsibilities are, if we compare the derivative with respect to $$\mu$$ of the simple univariate Gaussian $$d \mathcal{L} / d \mu$$, and the partial derivative of $$\mu_{k}$$ of the univariate GMM $$\partial \mathcal{L} / \partial \mu_{k}$$, given by

$$
\frac{d \mathcal{L}}{d \mu} =  \sum_{n=1}^{N} \frac{x_{n}-\mu}{\sigma^{2}},
\quad \text{and} \quad
\frac{\partial \mathcal{L}}{\partial \mu_{k}} = \sum_{n=1}^{N} 
\underbrace{
\frac{\pi_{k} \mathcal{N}\left(x_{n} \mid \mu_{k}, \sigma_{k}\right)}{\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left(x_{n} \mid \mu_{j}, \sigma_{j}\right)} 
}
_{\text{responsibilities}}
\cdot \frac{x_{n}-\mu_{k}}{\sigma_{k}^{2}}.
$$

As you can see the two terms are almost identical. The additional factor in the GMM derivative is what we call responsibilities. More formally, the responsibility $$r_{nk}$$ for the $$k$$-th component and the $$n$$-th data point is defined as:

$$
r_{nk} = \frac{\pi_{k} \mathcal{N}\left(x_{n} \mid \mu_{k}, \sigma_{k}\right)}{\sum_{j=1}^{K} \pi_{j} \mathcal{N}\left(x_{n} \mid \mu_{j}, \sigma_{j}\right)}.
$$

Note that $$ r_{nk} \propto  \pi_{k} \mathcal{N}\left(x_{n} \mid \mu_{k}, \sigma_{k}\right)$$, meaning that the $$k$$-th mixture components has a high responsibility for a data point $$x_{n}$$ when the data point is a plausible sample from that component. Note also that $$ r_{n}:=\left[r_{n 1}, \ldots, r_{n K}\right]^{\top} \in \mathbb{R}^{K} $$ is a probability vector since the individual responsibilities sum up to 1 due to the constraint on $$\pi$$. You can consider this vector as a weighted assignment of a point to the $$k$$ components. Responsibilities can be arranged in a matrix $$\in \mathbb{R}^{N \times K}$$. The total responsibility of the $$k$$-th mixture component for the entire dataset is defined as

$$
N_{k} = \sum_{n=1}^{N} r_{n k}.
$$

Using both the responsibilities and the $$N_{k}$$ notation, we can express in a compact way the equations for $$\mu_{k}$$, $$\sigma_{k}$$, and $$\pi_{k}$$ in the **univariate** case:

$$
\mu_{k} =\frac{1}{N_{k}} \sum_{n=1}^{N} r_{n k} x_{n}, \quad
\sigma_{k} =\frac{1}{N_{k}} \sum_{n=1}^{N} r_{n k}\left(x_{n}-\mu_{k}\right)^{2}, \quad
\pi_{k} =\frac{N_{k}}{N},
$$

and similarly the equations for $$\boldsymbol{\mu}_{k}$$, $$\boldsymbol{\Sigma}_{k}$$, and $$\pi_{k}$$ in the **multivariate** case:

$$
\boldsymbol{\mu}_{k} =\frac{1}{N_{k}} \sum_{n=1}^{N} r_{n k} \boldsymbol{x}_{n}, \quad
\boldsymbol{\Sigma}_{k} =\frac{1}{N_{k}} \sum_{n=1}^{N} r_{n k}\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{k}\right)\left(\boldsymbol{x}_{n}-\boldsymbol{\mu}_{k}\right)^{\top}, \quad
\pi_{k} =\frac{N_{k}}{N}.
$$

Responsibilities are useful for better formalizing the problem, but they are also the reason why we cannot estimate a closed-form expression of the ML estimator for a GMM.  Responsibilities impose a set of interlocking equations over means, (co)variances, and mixture weights, with the solution to the parameters requiring the values of the responsibilities (and vice versa). However, the conceptual separation of parameters and responsibilities suggests the possibility of an iterative methods based on two steps: (i) update the parameters (keeping the responsibilities fixed), and (ii) estimate the responsibilities (keeping the parameters fixed). Iterating over these two steps will eventually reach an optimum. It turns out that this solution is a particular instance of the Expectation Maximization algorithm.



Expectation Maximization (EM) for GMMs
--------------------------------------

The Expectation Maximization (EM) algorithm has been proposed by Dempster et al. (1977) as an iterative method for finding the maximum likelihood (or maximum a posteriori, MAP) estimates of a set of parameters. At its core, the algorithm consists of two step: the **E-step** in which a function for the expectation of the log-likelihood is computed based on the current parameters, and an **M-step** where the parameters found in the first step are maximized.

Every EM iteration increases the log-likelihood function. For convergence, we can check the log-likelihood and stop the algorithm when a certain threshold $$\epsilon$$ is reached, or alternatively after a predefined number of steps. Note that the solution is a **local optimum** and is strictly dependent from the initial values of the parameters. The algorithm can be summarized as follows:


1. **Init**: initialize the parameters $$\mu_k, \pi_k, \sigma_k$$ to random values.
2. **E-step**: using current values of $$\mu_k, \pi_k, \sigma_k$$ evaluate responsibilities $$r_{nk}$$.
3. **M-step**: using responsibilities found in 2 evaluate new $$\mu_k, \pi_k$$, and $$\sigma_k$$.
4. **Check**: verify stopping condition (e.g. log-likeliood $$<\epsilon$$), otherwise repeat from step 2.

Example: fitting distributions by GMMs
----------------------------------------


Bonus: Mixture Density Networks (MDNs)
------------------------------------------------

It would be interesting to find a way to mix GMMs with neural networks. For instance, we could have a neural network providing as output the parameters of the GMM. More precisely, the neural network could have different sets of outputs to represent means, variances, and mixture coefficients. What's the advantage? The output of the neural network will change the underlying mixture of Gaussians based on the input data. A standard GMM with $$K=2$$ will struggle in modeling a distribution with 10 modes, whereas the neural network will do a good job by adjusting the mixture based on the location of the inputs.

This type of neural networks has been proposed by Bishop (1994) under the name of **Mixture Density Networks (MDNs)**.



Conclusions
------------




References
------------

Bishop, C. M. (1994). Mixture density networks.

Deisenroth, M. P., Faisal, A. A., & Ong, C. S. (2020). Mathematics for machine learning. Cambridge University Press.

Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1), 1-22.

Heinz G, Peterson LJ, Johnson RW, Kerk CJ. (2003). Exploring Relationships in Body Dimensions. Journal of Statistics Education 11(2). 
