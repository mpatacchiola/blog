---
layout: post
title:  "An overview of Gaussian Mixture Models"
date:   2020-07-23 09:00:00 +0000
description: Quick overview of Gaussian Mixture Models (GMMs) with an intuitive introduction and python examples.
author: Massimiliano Patacchiola
type: bayesian methods
comments: false
published: false
---


In this post I will give you an overview of Gaussian Mixture Models (GMMs) starting from an intuitive introduction with some examples, passing through the mathematical details, and concluding with a Python implementation. This post is based on Chapter 11 of the book  *"Mathematics for Machine Learning"* by Deisenroth, Faisal, and Ong which is available in PDF [here](https://mml-book.com) and in the paperback version [here](https://www.cambridge.org/gb/academic/subjects/computer-science/pattern-recognition-and-machine-learning/mathematics-machine-learning?format=PB).

![Mathematics for Machine Learning book]({{site.baseurl}}/images/books_math_for_machine_learning.png){:class="img-responsive"}

**What do you need to know?** In order to enjoy the post you need to know some basic probability theory (random variables, probability distributions, etc), some calculus (derivatives and gradients), and some Python if you want to delve into the programming part. The post follows this plot:

- A quick recap on Gaussian distributions
- Fitting a Gaussian via Maximum Likelihood Estimation (MLE)
- Problem: a single Gaussian cannot fit multimodal data
- Solution: use Gaussian Mixture Models (GMMs)
- Problem: MLE cannot be used on GMMs
- Solution: use Expectation Maximization (EM)


Quick recap on Gaussian distributions
-------------------------------------

Since we are going to extensively use Gaussian distributions I will present here a quick recap. A Gaussian distribution is a continuous [probability distribution](https://en.wikipedia.org/wiki/Probability_distribution) that is characterized by its symmetrical bell shape. A **univariate Gaussian** distribution is defined as follows:

$$
p\left(x \mid \mu, \sigma^{2}\right)= \mathcal{N}\left(\mu, \sigma^{2}\right) = \frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right).
$$

Note that $$\mu$$ and $$\sigma$$ are scalars representing the mean and standard deviation of the distribution. 
We like Gaussians because they have several nice properties, for instance marginals and conditionals of Gaussians are still Gaussians. Thanks to these properties Gaussian distributions have been widely used in a variety of algorithms and methods, such as the [Kalman filter](https://en.wikipedia.org/wiki/Kalman_filter) and [Gaussian processes](https://en.wikipedia.org/wiki/Gaussian_process).

The univariate Gaussian defines a distribution over a single random variable, but in many problems we have multiple random variables thus we need a version of the Gaussian which is able to deal with this multivariate case. The **multivariate Gaussian** distribution can be defined as follows:

$$
p(\boldsymbol{x} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \mathcal{N}\left(\boldsymbol{\mu}, \boldsymbol{\Sigma}\right) = (2 \pi)^{-\frac{D}{2}}|\boldsymbol{\Sigma}|^{-\frac{1}{2}} \exp \left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right).
$$

Note that $$\boldsymbol{\mu}$$ and $$\boldsymbol{\Sigma}$$ are not scalars but a vector (of means) and a matrix (of variances). 
The value $$|\boldsymbol{\Sigma}|$$ is the [determinant](https://en.wikipedia.org/wiki/Determinant) of $$\boldsymbol{\Sigma}$$, and $$D$$ is the number of dimensions $$\boldsymbol{x} \in \mathbb{R}^{D}$$. Interpolating over $$\boldsymbol{\mu}$$ has the effect of shifting the Gaussian on the $$D$$-dimensional (hyper)plane, whereas changing the matrix $$\boldsymbol{\Sigma}$$ has the effect of changing the shape of the Gaussian.

Example: fitting a Gaussian distribution
----------------------------------------

We are given a bunch of data representing the height of a group of people. We are interested in finding a distribution that fits this data. We can assume that the data has been generated by an underlying process, and that we want to model this process.

We can find mean and variance of a Gaussian via Maximum Likelihood Estimation (MLE), so if we have collected the height of a group of people we can use MLE to find the Gaussian that better fit the data. 


Problem: fitting a multimodal distribution
----------------------------------------


Solution: Gaussian Mixture Models
--------------------------------------

Here is an idea, what if instead of using a single Gaussian distribution we use multiple Gaussians instead? A GMM with $$K$$ components is defined as

$$
p(\boldsymbol{x} \mid \boldsymbol{\theta}) = \mathcal{N}\left(\boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k} \right) = \sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\boldsymbol{x} \mid \boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}\right)
\quad \text{where} 
$$

$$
0 \leqslant \pi_{k} \leqslant 1, \quad \sum_{k=1}^{K} \pi_{k}=1
\quad \text{and} \quad
\boldsymbol{\theta}=\left\{\boldsymbol{\mu}_{k}, \boldsymbol{\Sigma}_{k}, \pi_{k} \right\}_{k=1}^{K}
$$

**Important**: GMMs are the weighted sum of Gaussian densities. This is different from the weighted sum of Gaussian random variables. For instance, given two Gaussian random variables $$\boldsymbol{x}$$ and $$\boldsymbol{y}$$, their weighted sum is defined as

$$
p(a \boldsymbol{x}+b \boldsymbol{y})=\mathcal{N}\left(a \boldsymbol{\mu}_{x}+b \boldsymbol{\mu}_{y}, a^{2} \boldsymbol{\Sigma}_{x}+b^{2} \boldsymbol{\Sigma}_{y}\right).
$$



Conclusions
------------




References
------------

Deisenroth, M. P., Faisal, A. A., & Ong, C. S. (2020). Mathematics for machine learning. Cambridge University Press.


